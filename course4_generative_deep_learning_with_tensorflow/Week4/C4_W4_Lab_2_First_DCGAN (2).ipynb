{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "C4_W4_Lab_2_First_DCGAN.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYWfJQBY5Ufy"
      },
      "source": [
        "# Ungraded Lab: First DCGAN\n",
        "\n",
        "In this lab, you will see a demo of a Deep Convolutional GAN (DCGAN) trained on Fashion MNIST. You'll see architectural differences from the GAN in the first lab and also see the best practices when building this network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xu2WWncO6ElL"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H06EKcnhxLcM"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import urllib.request\n",
        "import zipfile\n",
        "from IPython import display"
      ],
      "metadata": {
        "id": "LZFCwnYlL7y2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWTkli2k6GGd"
      },
      "source": [
        "## Utilities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91DeiLbpxv5_"
      },
      "source": [
        "def plot_results(images, n_cols=None):\n",
        "    '''visualizes fake images'''\n",
        "    display.clear_output(wait=False)  \n",
        "\n",
        "    n_cols = n_cols or len(images)\n",
        "    n_rows = (len(images) - 1) // n_cols + 1\n",
        "\n",
        "    if images.shape[-1] == 1:\n",
        "        images = np.squeeze(images, axis=-1)\n",
        "\n",
        "    plt.figure(figsize=(n_cols, n_rows))\n",
        "    \n",
        "    for index, image in enumerate(images):\n",
        "        plt.subplot(n_rows, n_cols, index + 1)\n",
        "        plt.imshow(image, cmap=\"binary\")\n",
        "        plt.axis(\"off\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXjx6s2z6IiY"
      },
      "source": [
        "## Download and Prepare the Dataset\n",
        "\n",
        "You will use the [Fashion MNIST](https://github.com/zalandoresearch/fashion-mnist) dataset for this exercise. As before, you will only need to create batches of the training images. The preprocessing steps are also shown below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jl5uWcOMxg8A"
      },
      "source": [
        "# download the training images\n",
        "(X_train, _), _ = keras.datasets.fashion_mnist.load_data()\n",
        "\n",
        "# normalize pixel values\n",
        "X_train = X_train.astype(np.float32) / 255\n",
        "\n",
        "# reshape and rescale\n",
        "X_train = X_train.reshape(-1, 28, 28, 1) * 2. - 1.\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "# create batches of tensors to be fed into the model\n",
        "dataset = tf.data.Dataset.from_tensor_slices(X_train)\n",
        "dataset = dataset.shuffle(1000)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True).prefetch(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# download the dataset\n",
        "training_url = \"https://storage.googleapis.com/laurencemoroney-blog.appspot.com/Resources/signs-training.zip\"\n",
        "training_file_name = \"signs-training.zip\"\n",
        "urllib.request.urlretrieve(training_url, training_file_name)\n",
        "\n",
        "# extract to local directory\n",
        "training_dir = \"/tmp\"\n",
        "zip_ref = zipfile.ZipFile(training_file_name, 'r')\n",
        "zip_ref.extractall(training_dir)\n",
        "zip_ref.close()"
      ],
      "metadata": {
        "id": "8g97EWXQL3Oc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 32\n",
        "\n",
        "# mapping function for preprocessing the image files\n",
        "def map_images(file):\n",
        "  '''converts the images to floats and normalizes the pixel values'''\n",
        "  img = tf.io.decode_png(tf.io.read_file(file))\n",
        "  img = tf.dtypes.cast(img, tf.float32)\n",
        "  img = img / 255.0\n",
        "  \n",
        "  return img\n",
        "\n",
        "# create training batches\n",
        "filename_dataset = tf.data.Dataset.list_files(\"/tmp/signs-training/*.png\")\n",
        "image_dataset = filename_dataset.map(map_images).batch(BATCH_SIZE)"
      ],
      "metadata": {
        "id": "aozvQgK0MBqq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKsQkQun6MYE"
      },
      "source": [
        "## Build the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4wRwi6BEojs"
      },
      "source": [
        "In DCGANs, convolutional layers are predominantly used to build the generator and discriminator. You will see how the layers are stacked as well as the best practices shown below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHfJkWHLGzcC"
      },
      "source": [
        "### Generator\n",
        "\n",
        "For the generator, we take in random noise and eventually transform it to the shape of the Fashion MNIST images. The general steps are:\n",
        "\n",
        "* Feed the input noise to a dense layer.\n",
        "* Reshape the output to have three dimensions. This stands for the (length, width, number of filters).\n",
        "* Perform a deconvolution (with [Conv2DTranspose](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2DTranspose)), reducing the number of filters by half and using a stride of `2`.\n",
        "* The final layer upsamples the features to the size of the training images. In this case 28 x 28 x 1.\n",
        "\n",
        "Notice that batch normalization is performed except for the final deconvolution layer. As best practice, `selu` is the activation used for the intermediate deconvolution while `tanh` is for the output. We printed the model summary so you can see the shapes at each layer.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWnY7jRxfboU"
      },
      "source": [
        "codings_size = 32\n",
        "\n",
        "generator = keras.models.Sequential([\n",
        "    keras.layers.Dense(7 * 7 * 128, input_shape=[codings_size]),\n",
        "    keras.layers.Reshape([7, 7, 128]),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Conv2DTranspose(64, kernel_size=5, strides=2, padding=\"SAME\",\n",
        "                                 activation=\"selu\"),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Conv2DTranspose(1, kernel_size=5, strides=2, padding=\"SAME\",\n",
        "                                 activation=\"tanh\"),\n",
        "])\n",
        "\n",
        "generator.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08aoWcgALISR"
      },
      "source": [
        "As a sanity check, let's see the fake images generated by the untrained generator and see the dimensions of the output."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZ7KPXTp_49h"
      },
      "source": [
        "# generate a batch of noise input (batch size = 16)\n",
        "test_noise = tf.random.normal([16, codings_size])\n",
        "\n",
        "# feed the batch to the untrained generator\n",
        "test_image = generator(test_noise)\n",
        "\n",
        "# visualize sample output\n",
        "plot_results(test_image, n_cols=4)\n",
        "\n",
        "print(f'shape of the generated batch: {test_image.shape}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ayaQOu0Ll1M"
      },
      "source": [
        "### Discriminator\n",
        "\n",
        "The discriminator will use strided convolutions to reduce the dimensionality of the input images. As best practice, these are activated by [LeakyRELU](https://keras.io/api/layers/activation_layers/leaky_relu/). The output features will be flattened and fed to a 1-unit dense layer activated by `sigmoid`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GRpxEFCQ_J0x"
      },
      "source": [
        "discriminator = keras.models.Sequential([\n",
        "    keras.layers.Conv2D(64, kernel_size=5, strides=2, padding=\"SAME\",\n",
        "                        activation=keras.layers.LeakyReLU(0.2),\n",
        "                        input_shape=[28, 28, 1]),\n",
        "    keras.layers.Dropout(0.4),\n",
        "    keras.layers.Conv2D(128, kernel_size=5, strides=2, padding=\"SAME\",\n",
        "                        activation=keras.layers.LeakyReLU(0.2)),\n",
        "    keras.layers.Dropout(0.4),\n",
        "    keras.layers.Flatten(),\n",
        "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
        "])\n",
        "\n",
        "discriminator.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqJIlaNUMwNT"
      },
      "source": [
        "As before, you will append these two subnetwork to build the complete GAN."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qIuauNaX_LUd"
      },
      "source": [
        "gan = keras.models.Sequential([generator, discriminator])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gd8o7Orz6jS7"
      },
      "source": [
        "## Configure the Model for training\n",
        "\n",
        "The discriminator and GAN will still be classifying fake and real images so you will use the same settings as before."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pOy61losflZ-"
      },
      "source": [
        "discriminator.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\")\n",
        "discriminator.trainable = False\n",
        "gan.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5XWApPx6PqB"
      },
      "source": [
        "## Train the Model\n",
        "\n",
        "The training loop will also be identical to the previous one you built. Run the cells below and observe how the fake images become more convincing as the training progresses."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cXWf5qn5xCeQ"
      },
      "source": [
        "def train_gan(gan, dataset, random_normal_dimensions, n_epochs=50):\n",
        "    \"\"\" Defines the two-phase training loop of the GAN\n",
        "    Args:\n",
        "      gan -- the GAN model which has the generator and discriminator\n",
        "      dataset -- the training set of real images\n",
        "      random_normal_dimensions -- dimensionality of the input to the generator\n",
        "      n_epochs -- number of epochs\n",
        "    \"\"\"\n",
        "    generator, discriminator = gan.layers\n",
        "    for epoch in range(n_epochs):\n",
        "        print(\"Epoch {}/{}\".format(epoch + 1, n_epochs))       \n",
        "        for real_images in dataset:\n",
        "            # infer batch size from the training batch\n",
        "            batch_size = real_images.shape[0]\n",
        "\n",
        "            # Train the discriminator - PHASE 1\n",
        "            # create the noise\n",
        "            noise = tf.random.normal(shape=[batch_size, random_normal_dimensions])\n",
        "\n",
        "            # use the noise to generate fake images\n",
        "            fake_images = generator(noise)\n",
        "\n",
        "            # create a list by concatenating the fake images with the real ones\n",
        "            mixed_images = tf.concat([fake_images, real_images], axis=0)\n",
        "\n",
        "            # Create the labels for the discriminator\n",
        "            # 0 for the fake images\n",
        "            # 1 for the real images\n",
        "            discriminator_labels = tf.constant([[0.]] * batch_size + [[1.]] * batch_size)\n",
        "\n",
        "            # ensure that the discriminator is trainable\n",
        "            discriminator.trainable = True\n",
        "\n",
        "            # use train_on_batch to train the discriminator with the mixed images and the discriminator labels\n",
        "            discriminator.train_on_batch(mixed_images, discriminator_labels)\n",
        "\n",
        "            # Train the generator - PHASE 2\n",
        "            # create a batch of noise input to feed to the GAN\n",
        "            noise = tf.random.normal(shape=[batch_size, random_normal_dimensions])\n",
        "            \n",
        "            # label all generated images to be \"real\"\n",
        "            generator_labels = tf.constant([[1.]] * batch_size)\n",
        "\n",
        "            # freeze the discriminator\n",
        "            discriminator.trainable = False\n",
        "\n",
        "            # train the GAN on the noise with the labels all set to be true\n",
        "            gan.train_on_batch(noise, generator_labels)\n",
        "        \n",
        "        # plot the fake images used to train the discriminator\n",
        "        plot_results(fake_images, 16)                     \n",
        "        plt.show()      \n",
        "    return fake_images"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eKsXGD_NfqPJ"
      },
      "source": [
        "fake_images =train_gan(gan, image_dataset, codings_size, 20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate a batch of noise input (batch size = 16)\n",
        "test_noise = tf.random.normal([96, codings_size])\n",
        "\n",
        "# feed the batch to the untrained generator\n",
        "test_images = generator(test_noise)"
      ],
      "metadata": {
        "id": "0q8XrtxxdrHk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# helper function to collect the images\n",
        "def append_to_grading_images(images, indexes):\n",
        "  l = []\n",
        "  for index in indexes:\n",
        "    if len(l) >= 16:\n",
        "      print(\"The list is full\")\n",
        "      break\n",
        "    l.append(tf.squeeze(images[index:(index+1),...], axis=0))\n",
        "  l = tf.convert_to_tensor(l)\n",
        "  return l"
      ],
      "metadata": {
        "id": "Esn9DH6hM7ZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(list(range(0, 16)))\n",
        "print(list(range(16, 32)))"
      ],
      "metadata": {
        "id": "eiXISoaGdB2W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grading_images = append_to_grading_images(fake_images, [0, 1, 3, 4, 8, 9, 10, 11, 12, 14, 15, 16, 20, 22, 24, 25])"
      ],
      "metadata": {
        "id": "eSpKWc8wM9y2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grading_images.shape"
      ],
      "metadata": {
        "id": "WaIrwx4lUS4W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "from zipfile import ZipFile\n",
        "\n",
        "denormalized_images = grading_images * 255\n",
        "denormalized_images = tf.dtypes.cast(denormalized_images, dtype = tf.uint8)\n",
        "\n",
        "file_paths = []\n",
        "\n",
        "for this_image in range(0,16):\n",
        "  i = tf.reshape(denormalized_images[this_image], [28,28])\n",
        "  im = Image.fromarray(i.numpy())\n",
        "  im = im.convert(\"L\")\n",
        "  filename = \"hand\" + str(this_image) + \".png\"\n",
        "  file_paths.append(filename)\n",
        "  im.save(filename)\n",
        "\n",
        "with ZipFile('my-signs.zip', 'w') as zip:\n",
        "  for file in file_paths:\n",
        "    zip.write(file)"
      ],
      "metadata": {
        "id": "geM2RR3_NAdn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hi=5"
      ],
      "metadata": {
        "id": "ntqa51uaO0vM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "8oQ6eaQCeDcK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "iztN3J2FPcE0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}